{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\mukilan t\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\mukilan t\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mukilan t\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.8.1)\n",
      "Requirement already satisfied: namex in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\mukilan t\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\mukilan t\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow numpy matplotlib opencv-python scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data (SAR and optical)\n",
      "Executing load_dataset_paths for base_path: v_2\n",
      "Splitting the data into train and validation sets\n",
      "Defining and training the model\n",
      "Executing advanced_unet_with_fusion\n",
      "Executing se_block\n",
      "Executing se_block\n",
      "Executing se_block\n",
      "Executing se_block\n",
      "Training the model\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "`output_signature` must contain objects that are subclass of `tf.TypeSpec` but found <class 'list'> which is not.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 134\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining the model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 134\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving the trained model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Mukilan T\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Mukilan T\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py:124\u001b[0m, in \u001b[0;36m_from_generator\u001b[1;34m(generator, output_types, output_shapes, args, output_signature, name)\u001b[0m\n\u001b[0;32m    122\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m spec \u001b[38;5;129;01min\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mflatten(output_signature):\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec, type_spec\u001b[38;5;241m.\u001b[39mTypeSpec):\n\u001b[1;32m--> 124\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`output_signature` must contain objects that are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    125\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubclass of `tf.TypeSpec` but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(spec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    126\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich is not.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m output_types \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: `output_signature` must contain objects that are subclass of `tf.TypeSpec` but found <class 'list'> which is not."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Input, Add, Multiply, Concatenate, UpSampling2D, Dense, GlobalAveragePooling2D, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Custom data generator\n",
    "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, sar_files, optical_files, batch_size, img_size=(256, 256)):\n",
    "        self.sar_files = sar_files\n",
    "        self.optical_files = optical_files\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.indices = np.arange(len(self.sar_files))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.sar_files) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        sar_batch = [self.sar_files[i] for i in batch_indices]\n",
    "        optical_batch = [self.optical_files[i] for i in batch_indices]\n",
    "        return self.__data_generation(sar_batch, optical_batch)\n",
    "\n",
    "    def __data_generation(self, sar_batch, optical_batch):\n",
    "        sar_images = np.array([img_to_array(load_img(file, target_size=self.img_size)) / 255.0 for file in sar_batch])\n",
    "        optical_images = np.array([img_to_array(load_img(file, target_size=self.img_size)) / 255.0 for file in optical_batch])\n",
    "        return [tf.convert_to_tensor(sar_images), tf.convert_to_tensor(optical_images)], tf.convert_to_tensor(optical_images)\n",
    "\n",
    "# Load dataset file paths\n",
    "def load_dataset_paths(base_path):\n",
    "    print(f\"Executing load_dataset_paths for base_path: {base_path}\")\n",
    "    sar_files = []\n",
    "    optical_files = []\n",
    "    for category in ['agri', 'barrenland', 'grassland', 'urban']:\n",
    "        category_path = os.path.join(base_path, category)\n",
    "        sar_path = os.path.join(category_path, 's1')\n",
    "        optical_path = os.path.join(category_path, 's2')\n",
    "        \n",
    "        sar_files.extend([os.path.join(sar_path, file) for file in os.listdir(sar_path)])\n",
    "        optical_files.extend([os.path.join(optical_path, file) for file in os.listdir(optical_path)])\n",
    "    \n",
    "    return sar_files, optical_files\n",
    "\n",
    "# Squeeze-and-Excitation (SE) Block\n",
    "def se_block(input_tensor, reduction=16):\n",
    "    print(\"Executing se_block\")\n",
    "    filters = input_tensor.shape[-1]\n",
    "    se = GlobalAveragePooling2D()(input_tensor)\n",
    "    se = Dense(filters // reduction, activation='relu')(se)\n",
    "    se = Dense(filters, activation='sigmoid')(se)\n",
    "    se = Reshape((1, 1, filters))(se)\n",
    "    se = Multiply()([input_tensor, se])\n",
    "    return se\n",
    "\n",
    "# Attention Block\n",
    "def attention_block(input_tensor, gating_tensor):\n",
    "    print(\"Executing attention_block\")\n",
    "    filters = input_tensor.shape[-1]\n",
    "    gating = Conv2D(filters, 1, activation='relu', padding='same')(gating_tensor)\n",
    "    gating = Conv2D(filters, 1, activation='sigmoid', padding='same')(gating)\n",
    "    attention = Multiply()([input_tensor, gating])\n",
    "    return attention\n",
    "\n",
    "# Define Advanced U-Net with Fusion and Attention\n",
    "def advanced_unet_with_fusion(input_shape):\n",
    "    print(\"Executing advanced_unet_with_fusion\")\n",
    "    inputs_sar = Input(shape=input_shape)\n",
    "    inputs_optical = Input(shape=input_shape)  # Optical data\n",
    "    \n",
    "    # Fusion Layer\n",
    "    fusion = Concatenate()([inputs_sar, inputs_optical])\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(fusion)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    conv1 = se_block(conv1)\n",
    "    pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    conv2 = se_block(conv2)\n",
    "    pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    # Bottleneck\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    conv3 = se_block(conv3)\n",
    "    \n",
    "    # Decoder with Attention\n",
    "    up4 = UpSampling2D(size=(2, 2))(conv3)\n",
    "    concat4 = Concatenate()([up4, conv2])\n",
    "    conv4 = Conv2D(128, 3, activation='relu', padding='same')(concat4)\n",
    "    conv4 = Conv2D(128, 3, activation='relu', padding='same')(conv4)\n",
    "    conv4 = se_block(conv4)\n",
    "    \n",
    "    up5 = UpSampling2D(size=(2, 2))(conv4)\n",
    "    concat5 = Concatenate()([up5, conv1])\n",
    "    conv5 = Conv2D(64, 3, activation='relu', padding='same')(concat5)\n",
    "    conv5 = Conv2D(64, 3, activation='relu', padding='same')(conv5)\n",
    "    \n",
    "    # Output: LAB color space (3 channels: L, A, B)\n",
    "    outputs = Conv2D(3, 1, activation='tanh')(conv5)\n",
    "    \n",
    "    model = Model(inputs=[inputs_sar, inputs_optical], outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Load the data (SAR and optical)\n",
    "print(\"Loading the data (SAR and optical)\")\n",
    "base_path = 'v_2'\n",
    "sar_files, optical_files = load_dataset_paths(base_path)\n",
    "\n",
    "# Train/test split\n",
    "print(\"Splitting the data into train and validation sets\")\n",
    "X_train_sar, X_val_sar, X_train_optical, X_val_optical = train_test_split(\n",
    "    sar_files, optical_files, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define and train the model\n",
    "print(\"Defining and training the model\")\n",
    "input_shape = (256, 256, 3)  # Assuming RGB images of size 256x256\n",
    "model = advanced_unet_with_fusion(input_shape)\n",
    "\n",
    "# Create data generators\n",
    "batch_size = 16\n",
    "train_generator = CustomDataGenerator(X_train_sar, X_train_optical, batch_size)\n",
    "val_generator = CustomDataGenerator(X_val_sar, X_val_optical, batch_size)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model\")\n",
    "model.fit(train_generator, validation_data=val_generator, epochs=20)\n",
    "\n",
    "# Save the trained model\n",
    "print(\"Saving the trained model\")\n",
    "model.save('advanced_sar_colorization_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Input, Add, Multiply, Concatenate, UpSampling2D, Dense, GlobalAveragePooling2D, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Custom data generator\n",
    "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, sar_files, optical_files, batch_size, img_size=(256, 256)):\n",
    "        self.sar_files = sar_files\n",
    "        self.optical_files = optical_files\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.indices = np.arange(len(self.sar_files))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.sar_files) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        sar_batch = [self.sar_files[i] for i in batch_indices]\n",
    "        optical_batch = [self.optical_files[i] for i in batch_indices]\n",
    "        return self.__data_generation(sar_batch, optical_batch)\n",
    "\n",
    "    def __data_generation(self, sar_batch, optical_batch):\n",
    "        sar_images = np.array([img_to_array(load_img(file, target_size=self.img_size)) / 255.0 for file in sar_batch])\n",
    "        optical_images = np.array([img_to_array(load_img(file, target_size=self.img_size)) / 255.0 for file in optical_batch])\n",
    "        return [tf.convert_to_tensor(sar_images), tf.convert_to_tensor(optical_images)], tf.convert_to_tensor(optical_images)\n",
    "\n",
    "# Load dataset file paths\n",
    "def load_dataset_paths(base_path):\n",
    "    print(f\"Executing load_dataset_paths for base_path: {base_path}\")\n",
    "    sar_files = []\n",
    "    optical_files = []\n",
    "    for category in ['agri', 'barrenland', 'grassland', 'urban']:\n",
    "        category_path = os.path.join(base_path, category)\n",
    "        sar_path = os.path.join(category_path, 's1')\n",
    "        optical_path = os.path.join(category_path, 's2')\n",
    "        \n",
    "        sar_files.extend([os.path.join(sar_path, file) for file in os.listdir(sar_path)])\n",
    "        optical_files.extend([os.path.join(optical_path, file) for file in os.listdir(optical_path)])\n",
    "    \n",
    "    return sar_files, optical_files\n",
    "\n",
    "# Squeeze-and-Excitation (SE) Block\n",
    "def se_block(input_tensor, reduction=16):\n",
    "    print(\"Executing se_block\")\n",
    "    filters = input_tensor.shape[-1]\n",
    "    se = GlobalAveragePooling2D()(input_tensor)\n",
    "    se = Dense(filters // reduction, activation='relu')(se)\n",
    "    se = Dense(filters, activation='sigmoid')(se)\n",
    "    se = Reshape((1, 1, filters))(se)\n",
    "    se = Multiply()([input_tensor, se])\n",
    "    return se\n",
    "\n",
    "# Attention Block\n",
    "def attention_block(input_tensor, gating_tensor):\n",
    "    print(\"Executing attention_block\")\n",
    "    filters = input_tensor.shape[-1]\n",
    "    gating = Conv2D(filters, 1, activation='relu', padding='same')(gating_tensor)\n",
    "    gating = Conv2D(filters, 1, activation='sigmoid', padding='same')(gating)\n",
    "    attention = Multiply()([input_tensor, gating])\n",
    "    return attention\n",
    "\n",
    "# Define Advanced U-Net with Fusion and Attention\n",
    "def advanced_unet_with_fusion(input_shape):\n",
    "    print(\"Executing advanced_unet_with_fusion\")\n",
    "    inputs_sar = Input(shape=input_shape)\n",
    "    inputs_optical = Input(shape=input_shape)  # Optical data\n",
    "    \n",
    "    # Fusion Layer\n",
    "    fusion = Concatenate()([inputs_sar, inputs_optical])\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(fusion)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    conv1 = se_block(conv1)\n",
    "    pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    conv2 = se_block(conv2)\n",
    "    pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    # Bottleneck\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    conv3 = se_block(conv3)\n",
    "    \n",
    "    # Decoder with Attention\n",
    "    up4 = UpSampling2D(size=(2, 2))(conv3)\n",
    "    concat4 = Concatenate()([up4, conv2])\n",
    "    conv4 = Conv2D(128, 3, activation='relu', padding='same')(concat4)\n",
    "    conv4 = Conv2D(128, 3, activation='relu', padding='same')(conv4)\n",
    "    conv4 = se_block(conv4)\n",
    "    \n",
    "    up5 = UpSampling2D(size=(2, 2))(conv4)\n",
    "    concat5 = Concatenate()([up5, conv1])\n",
    "    conv5 = Conv2D(64, 3, activation='relu', padding='same')(concat5)\n",
    "    conv5 = Conv2D(64, 3, activation='relu', padding='same')(conv5)\n",
    "    \n",
    "    # Output: LAB color space (3 channels: L, A, B)\n",
    "    outputs = Conv2D(3, 1, activation='tanh')(conv5)\n",
    "    \n",
    "    model = Model(inputs=[inputs_sar, inputs_optical], outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Load the data (SAR and optical)\n",
    "print(\"Loading the data (SAR and optical)\")\n",
    "base_path = 'v_2'\n",
    "sar_files, optical_files = load_dataset_paths(base_path)\n",
    "\n",
    "# Train/test split\n",
    "print(\"Splitting the data into train and validation sets\")\n",
    "X_train_sar, X_val_sar, X_train_optical, X_val_optical = train_test_split(\n",
    "    sar_files, optical_files, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define and train the model\n",
    "print(\"Defining and training the model\")\n",
    "input_shape = (256, 256, 3)  # Assuming RGB images of size 256x256\n",
    "model = advanced_unet_with_fusion(input_shape)\n",
    "\n",
    "# Create data generators\n",
    "batch_size = 16\n",
    "train_generator = CustomDataGenerator(X_train_sar, X_train_optical, batch_size)\n",
    "val_generator = CustomDataGenerator(X_val_sar, X_val_optical, batch_size)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model\")\n",
    "model.fit(train_generator, validation_data=val_generator, epochs=20)\n",
    "\n",
    "# Save the trained model\n",
    "print(\"Saving the trained model\")\n",
    "model.save('advanced_sar_colorization_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Input, Add, Multiply, Concatenate, UpSampling2D, Dense, GlobalAveragePooling2D, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Custom data generator\n",
    "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, sar_files, optical_files, batch_size, img_size=(256, 256)):\n",
    "        self.sar_files = sar_files\n",
    "        self.optical_files = optical_files\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.indices = np.arange(len(self.sar_files))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.sar_files) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        sar_batch = [self.sar_files[i] for i in batch_indices]\n",
    "        optical_batch = [self.optical_files[i] for i in batch_indices]\n",
    "        return self.__data_generation(sar_batch, optical_batch)\n",
    "\n",
    "    def __data_generation(self, sar_batch, optical_batch):\n",
    "        sar_images = np.array([img_to_array(load_img(file, target_size=self.img_size)) / 255.0 for file in sar_batch])\n",
    "        optical_images = np.array([img_to_array(load_img(file, target_size=self.img_size)) / 255.0 for file in optical_batch])\n",
    "        return [tf.convert_to_tensor(sar_images), tf.convert_to_tensor(optical_images)], tf.convert_to_tensor(optical_images)\n",
    "\n",
    "# Load dataset file paths\n",
    "def load_dataset_paths(base_path):\n",
    "    print(f\"Executing load_dataset_paths for base_path: {base_path}\")\n",
    "    sar_files = []\n",
    "    optical_files = []\n",
    "    for category in ['agri', 'barrenland', 'grassland', 'urban']:\n",
    "        category_path = os.path.join(base_path, category)\n",
    "        sar_path = os.path.join(category_path, 's1')\n",
    "        optical_path = os.path.join(category_path, 's2')\n",
    "        \n",
    "        sar_files.extend([os.path.join(sar_path, file) for file in os.listdir(sar_path)])\n",
    "        optical_files.extend([os.path.join(optical_path, file) for file in os.listdir(optical_path)])\n",
    "    \n",
    "    return sar_files, optical_files\n",
    "\n",
    "# Squeeze-and-Excitation (SE) Block\n",
    "def se_block(input_tensor, reduction=16):\n",
    "    print(\"Executing se_block\")\n",
    "    filters = input_tensor.shape[-1]\n",
    "    se = GlobalAveragePooling2D()(input_tensor)\n",
    "    se = Dense(filters // reduction, activation='relu')(se)\n",
    "    se = Dense(filters, activation='sigmoid')(se)\n",
    "    se = Reshape((1, 1, filters))(se)\n",
    "    se = Multiply()([input_tensor, se])\n",
    "    return se\n",
    "\n",
    "# Attention Block\n",
    "def attention_block(input_tensor, gating_tensor):\n",
    "    print(\"Executing attention_block\")\n",
    "    filters = input_tensor.shape[-1]\n",
    "    gating = Conv2D(filters, 1, activation='relu', padding='same')(gating_tensor)\n",
    "    gating = Conv2D(filters, 1, activation='sigmoid', padding='same')(gating)\n",
    "    attention = Multiply()([input_tensor, gating])\n",
    "    return attention\n",
    "\n",
    "# Define Advanced U-Net with Fusion and Attention\n",
    "def advanced_unet_with_fusion(input_shape):\n",
    "    print(\"Executing advanced_unet_with_fusion\")\n",
    "    inputs_sar = Input(shape=input_shape)\n",
    "    inputs_optical = Input(shape=input_shape)  # Optical data\n",
    "    \n",
    "    # Fusion Layer\n",
    "    fusion = Concatenate()([inputs_sar, inputs_optical])\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(fusion)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    conv1 = se_block(conv1)\n",
    "    pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    conv2 = se_block(conv2)\n",
    "    pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    # Bottleneck\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    conv3 = se_block(conv3)\n",
    "    \n",
    "    # Decoder with Attention\n",
    "    up4 = UpSampling2D(size=(2, 2))(conv3)\n",
    "    concat4 = Concatenate()([up4, conv2])\n",
    "    conv4 = Conv2D(128, 3, activation='relu', padding='same')(concat4)\n",
    "    conv4 = Conv2D(128, 3, activation='relu', padding='same')(conv4)\n",
    "    conv4 = se_block(conv4)\n",
    "    \n",
    "    up5 = UpSampling2D(size=(2, 2))(conv4)\n",
    "    concat5 = Concatenate()([up5, conv1])\n",
    "    conv5 = Conv2D(64, 3, activation='relu', padding='same')(concat5)\n",
    "    conv5 = Conv2D(64, 3, activation='relu', padding='same')(conv5)\n",
    "    \n",
    "    # Output: LAB color space (3 channels: L, A, B)\n",
    "    outputs = Conv2D(3, 1, activation='tanh')(conv5)\n",
    "    \n",
    "    model = Model(inputs=[inputs_sar, inputs_optical], outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Load the data (SAR and optical)\n",
    "print(\"Loading the data (SAR and optical)\")\n",
    "base_path = 'v_2'\n",
    "sar_files, optical_files = load_dataset_paths(base_path)\n",
    "\n",
    "# Train/test split\n",
    "print(\"Splitting the data into train and validation sets\")\n",
    "X_train_sar, X_val_sar, X_train_optical, X_val_optical = train_test_split(\n",
    "    sar_files, optical_files, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define and train the model\n",
    "print(\"Defining and training the model\")\n",
    "input_shape = (256, 256, 3)  # Assuming RGB images of size 256x256\n",
    "model = advanced_unet_with_fusion(input_shape)\n",
    "\n",
    "# Create data generators\n",
    "batch_size = 16\n",
    "train_generator = CustomDataGenerator(X_train_sar, X_train_optical, batch_size)\n",
    "val_generator = CustomDataGenerator(X_val_sar, X_val_optical, batch_size)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model\")\n",
    "model.fit(train_generator, validation_data=val_generator, epochs=20)\n",
    "\n",
    "# Save the trained model\n",
    "print(\"Saving the trained model\")\n",
    "model.save('advanced_sar_colorization_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Input, Add, Multiply, Concatenate, UpSampling2D, Dense, GlobalAveragePooling2D, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Custom data generator\n",
    "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, sar_files, optical_files, batch_size, img_size=(256, 256)):\n",
    "        self.sar_files = sar_files\n",
    "        self.optical_files = optical_files\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.indices = np.arange(len(self.sar_files))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.sar_files) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        sar_batch = [self.sar_files[i] for i in batch_indices]\n",
    "        optical_batch = [self.optical_files[i] for i in batch_indices]\n",
    "        return self.__data_generation(sar_batch, optical_batch)\n",
    "\n",
    "    def __data_generation(self, sar_batch, optical_batch):\n",
    "        sar_images = np.array([img_to_array(load_img(file, target_size=self.img_size)) / 255.0 for file in sar_batch])\n",
    "        optical_images = np.array([img_to_array(load_img(file, target_size=self.img_size)) / 255.0 for file in optical_batch])\n",
    "        return [tf.convert_to_tensor(sar_images), tf.convert_to_tensor(optical_images)], tf.convert_to_tensor(optical_images)\n",
    "\n",
    "# Load dataset file paths\n",
    "def load_dataset_paths(base_path):\n",
    "    print(f\"Executing load_dataset_paths for base_path: {base_path}\")\n",
    "    sar_files = []\n",
    "    optical_files = []\n",
    "    for category in ['agri', 'barrenland', 'grassland', 'urban']:\n",
    "        category_path = os.path.join(base_path, category)\n",
    "        sar_path = os.path.join(category_path, 's1')\n",
    "        optical_path = os.path.join(category_path, 's2')\n",
    "        \n",
    "        sar_files.extend([os.path.join(sar_path, file) for file in os.listdir(sar_path)])\n",
    "        optical_files.extend([os.path.join(optical_path, file) for file in os.listdir(optical_path)])\n",
    "    \n",
    "    return sar_files, optical_files\n",
    "\n",
    "# Squeeze-and-Excitation (SE) Block\n",
    "def se_block(input_tensor, reduction=16):\n",
    "    print(\"Executing se_block\")\n",
    "    filters = input_tensor.shape[-1]\n",
    "    se = GlobalAveragePooling2D()(input_tensor)\n",
    "    se = Dense(filters // reduction, activation='relu')(se)\n",
    "    se = Dense(filters, activation='sigmoid')(se)\n",
    "    se = Reshape((1, 1, filters))(se)\n",
    "    se = Multiply()([input_tensor, se])\n",
    "    return se\n",
    "\n",
    "# Attention Block\n",
    "def attention_block(input_tensor, gating_tensor):\n",
    "    print(\"Executing attention_block\")\n",
    "    filters = input_tensor.shape[-1]\n",
    "    gating = Conv2D(filters, 1, activation='relu', padding='same')(gating_tensor)\n",
    "    gating = Conv2D(filters, 1, activation='sigmoid', padding='same')(gating)\n",
    "    attention = Multiply()([input_tensor, gating])\n",
    "    return attention\n",
    "\n",
    "# Define Advanced U-Net with Fusion and Attention\n",
    "def advanced_unet_with_fusion(input_shape):\n",
    "    print(\"Executing advanced_unet_with_fusion\")\n",
    "    inputs_sar = Input(shape=input_shape)\n",
    "    inputs_optical = Input(shape=input_shape)  # Optical data\n",
    "    \n",
    "    # Fusion Layer\n",
    "    fusion = Concatenate()([inputs_sar, inputs_optical])\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(fusion)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    conv1 = se_block(conv1)\n",
    "    pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    conv2 = se_block(conv2)\n",
    "    pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    # Bottleneck\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    conv3 = se_block(conv3)\n",
    "    \n",
    "    # Decoder with Attention\n",
    "    up4 = UpSampling2D(size=(2, 2))(conv3)\n",
    "    concat4 = Concatenate()([up4, conv2])\n",
    "    conv4 = Conv2D(128, 3, activation='relu', padding='same')(concat4)\n",
    "    conv4 = Conv2D(128, 3, activation='relu', padding='same')(conv4)\n",
    "    conv4 = se_block(conv4)\n",
    "    \n",
    "    up5 = UpSampling2D(size=(2, 2))(conv4)\n",
    "    concat5 = Concatenate()([up5, conv1])\n",
    "    conv5 = Conv2D(64, 3, activation='relu', padding='same')(concat5)\n",
    "    conv5 = Conv2D(64, 3, activation='relu', padding='same')(conv5)\n",
    "    \n",
    "    # Output: LAB color space (3 channels: L, A, B)\n",
    "    outputs = Conv2D(3, 1, activation='tanh')(conv5)\n",
    "    \n",
    "    model = Model(inputs=[inputs_sar, inputs_optical], outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Load the data (SAR and optical)\n",
    "print(\"Loading the data (SAR and optical)\")\n",
    "base_path = 'v_2'\n",
    "sar_files, optical_files = load_dataset_paths(base_path)\n",
    "\n",
    "# Train/test split\n",
    "print(\"Splitting the data into train and validation sets\")\n",
    "X_train_sar, X_val_sar, X_train_optical, X_val_optical = train_test_split(\n",
    "    sar_files, optical_files, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define and train the model\n",
    "print(\"Defining and training the model\")\n",
    "input_shape = (256, 256, 3)  # Assuming RGB images of size 256x256\n",
    "model = advanced_unet_with_fusion(input_shape)\n",
    "\n",
    "# Create data generators\n",
    "batch_size = 16\n",
    "train_generator = CustomDataGenerator(X_train_sar, X_train_optical, batch_size)\n",
    "val_generator = CustomDataGenerator(X_val_sar, X_val_optical, batch_size)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model\")\n",
    "model.fit(train_generator, validation_data=val_generator, epochs=20)\n",
    "\n",
    "# Save the trained model\n",
    "print(\"Saving the trained model\")\n",
    "model.save('advanced_sar_colorization_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
